{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1lQI1Qr09V4SmTnzASk7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghana-rathi/PONG/blob/master/GenAIsesh1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsLaiGLYT0ls",
        "outputId": "ac6c3e81-ed2f-4587-9328-f3a69abdd0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "yg03vc-aUkJ-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(messages):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages\n",
        "  )\n",
        "  return response.choices[0][\"message\"][\"content\"].strip()"
      ],
      "metadata": {
        "id": "BuwfrmU8U1OJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that respond friendly to any user input.\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6rXOeb9VBmc",
        "outputId": "d0a30da3-2d99-4e0f-b976-7ff69d776cae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that respond friendly to any user input.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you explain me what is a Large Language model?\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTDjCcp6VLWk",
        "outputId": "dcc4ef5d-ae7a-410d-fa26-5d1cb413e1e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to explain Large Language Models!\n",
            "\n",
            "A Large Language Model (LLM) is a type of artificial intelligence that uses deep learning techniques to process large amounts of natural language text and produce human-like responses or generate new text.\n",
            "\n",
            "LLMs are trained on vast datasets of textual data and use neural network architecture to model language patterns and relationships. They are capable of processing unstructured data, such as text obtained from the internet or books, and understanding natural language.\n",
            "\n",
            "One of the most well-known examples of an LLM is OpenAI's GPT-3 (Generative Pre-training Transformer 3) model. GPT-3 is capable of generating responses that are difficult to distinguish from those written by humans. LLMs such as GPT-3 have many potential applications, including natural language processing, chatbots, machine translation, and content creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"I want you to respond to emails \"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you explain me what is a Large Language model?\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyp06tYZVkiB",
        "outputId": "f3b32e11-fb58-4226-aa69-2316a6d96bbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! A large language model is a type of artificial intelligence that has been trained on a massive dataset of text or other language data. The goal of a large language model is to generate text that is coherent, relevant, and convincing, based on the input it receives.\n",
            "\n",
            "To create a large language model, researchers typically use machine learning algorithms to train the model on a large corpus of text, such as books, articles or web pages. The model analyzes the data and learns patterns in the language, which it can then use to generate new text that is similar in style and tone to the original corpus.\n",
            "\n",
            "Large language models have many practical applications, including:\n",
            "\n",
            "- Text generation\n",
            "- Translation\n",
            "- Sentiment analysis\n",
            "- Question answering\n",
            "- Chatbots\n",
            "\n",
            "Some examples of large language models include GPT-3 by OpenAI, BERT by Google, and RoBERTa by Facebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"I want you to act as an NLP Sentiment Analyzer of each sentence received from the user with a number from 0 to 1 in the format 'X.Y' where X is 0 or 1 and Y is an integer from 0 to 99\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a very bad review\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fub9Ql7VuzS",
        "outputId": "ef7fcafa-b4a3-41bf-ced4-bf7cd194e8f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment score of your statement is 0.12.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"I want you to act as an NLP Sentiment Analyzer of each sentence received from the user with a number from 0 to 1 in the format 'X.Y' where X is 0 or 1 and Y is an integer from 0 to 99\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a very bad review\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"0.0\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a very good review\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"1.0\"},\n",
        "    {\"role\": \"user\", \"content\": \"I did not like that much the movie\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hCjCnlyV2cO",
        "outputId": "c7aac014-f423-4c1e-8232-239c9e67b9cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"I want you to act as a text translator from English to Spanish. Whenever the input text is not in English, I want you to respond '$ERROR$'\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a sentence in English\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsnGHfdaWBwK",
        "outputId": "9f4d72f7-7eef-496e-f7b1-53756f3172e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esta es una oración en inglés.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"I want you to act as a text translator from English to Spanish. Whenever the input text is not in English, I want you to respond '$ERROR$'\"},\n",
        "    {\"role\": \"user\", \"content\": \"Esta es una frase en espanol\"}\n",
        "]\n",
        "response = chat(conversation)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jbl5GoWWKNa",
        "outputId": "ab258956-606f-4214-fb0e-a0a0bc329663"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ERROR$\n"
          ]
        }
      ]
    }
  ]
}